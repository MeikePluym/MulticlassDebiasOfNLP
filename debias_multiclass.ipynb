{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import copy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (MODEL_WITH_LM_HEAD_MAPPING, WEIGHTS_NAME, AdamW, AutoConfig, AutoModelWithLMHead, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer, get_linear_schedule_with_warmup)\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    #model architecture to be trained/fine-tuned\n",
    "    model_type='gpt2'   #['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "    #weights for loss function\n",
    "    alpha=0.2\n",
    "    beta=1-alpha\n",
    "    #number of attributes in attribute dict; always 1 (?)\n",
    "    num_attributes=1\n",
    "    #keep training from checkpoint; train next epoch (fine-tuning). False if from scratch\n",
    "    checkpoint=False\n",
    "\n",
    "    #input data file\n",
    "    data_files=['./data/'+model_type+'/gender_stereotype_data.bin', './data/'+model_type+'/ethnicities_polarized_class_data.bin',\n",
    "                './data/'+model_type+'/ethnicities_physical_data.bin', './data/'+model_type+'/religion_polarized_class_data.bin']\n",
    "    #data_file='./data/'+model_type+'/gender_stereotype_data.bin'\n",
    "    #data_file_ethnicities='./data/'+model_type+'/ethnicities_polarized_class_data.bin'\n",
    "\n",
    "    #output directory where model predictions/checkpoints will be written\n",
    "    output_dir='./debiased_models/'+model_type\n",
    "\n",
    "    #model checkpoint for weights initialization; None if you want to train model from scratch\n",
    "    if model_type=='bert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/bert/debiased-checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='bert-base-uncased'\n",
    "    elif model_type=='roberta':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/roberta/debiased-checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='roberta-base'\n",
    "    elif model_type=='albert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/albert/debiased-checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='albert-base-v2'\n",
    "    elif model_type=='dbert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/dbert/debiased-checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='distilbert-base-uncased'\n",
    "    elif model_type=='electra':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/electra/debiased-checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='google/electra-small-discriminator'\n",
    "    elif model_type=='gpt2':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/gpt2/debiased-checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='gpt2'\n",
    "\n",
    "    #optional pretrained config name/path if not same as model_name_or_path\n",
    "    config_name=''\n",
    "\n",
    "    #optional pretrained tokenizer name/path\n",
    "    tokenizer_name=''\n",
    "\n",
    "    #optional input evaluation data file\n",
    "    eval_data_file=None\n",
    "\n",
    "    #whether distinct lines of text in data are to be handled as distinct sequences\n",
    "    line_by_line=True\n",
    "\n",
    "    #whether to continue from latest checkpoint in output_dir\n",
    "    should_continue=False\n",
    "\n",
    "    #loss target; sentence or token\n",
    "    loss_target=\"sentence\"\n",
    "\n",
    "    #train with MLM loss instead of language modeling\n",
    "    mlm=False\n",
    "\n",
    "    #ratio of tokens to mask for MLM loss\n",
    "    mlm_probability=0.15\n",
    "\n",
    "    #optional directory to store pre-trained models\n",
    "    cache_dir=None\n",
    "\n",
    "    #optional input sequence length after tokenization; training data will be truncated. Default; max input length for single sentence inputs\n",
    "    block_size=128\n",
    "\n",
    "    #whether to run training\n",
    "    do_train=True\n",
    "    #whether to run eval\n",
    "    do_eval=True\n",
    "    #run eval during training at each logging step\n",
    "    evaluate_during_training=True\n",
    "\n",
    "    #batch size for train\n",
    "    per_gpu_train_batch_size=32\n",
    "    #bathc size for eval\n",
    "    per_gpu_eval_batch_size=32\n",
    "\n",
    "    #numb. of update steps to accumulate before performing backward/update pass\n",
    "    gradient_accumulation_steps=1\n",
    "    #initial learning rate for Adam\n",
    "    learning_rate=5e-5\n",
    "    #weight decay (if applied)\n",
    "    weight_decay=0.0\n",
    "    #epsilon for Adam optimizer\n",
    "    adam_epsilon= 1e-8\n",
    "    #max gradient norm.\n",
    "    max_grad_norm=1.0\n",
    "    #weighted loss\n",
    "    weighted_loss=[alpha, beta]\n",
    "    #['all', 'first', 'last'] to debias\n",
    "    debias_layer='all'\n",
    "    #number of training epochs\n",
    "    num_train_epochs=2\n",
    "    #if >0; set total number of training steps to perform\n",
    "    max_steps=-1\n",
    "    #linear warmup\n",
    "    warmup_steps=0\n",
    "    #square loss\n",
    "    square_loss=True\n",
    "    #token loss\n",
    "    token_loss=False\n",
    "    #log every x update steps\n",
    "    logging_steps=500\n",
    "    #save checkpoint every x update steps\n",
    "    save_steps=500\n",
    "    #limit total amount of checkpoints, delete older in output_dir\n",
    "    save_total_limit=None\n",
    "    #evaluate all checkpoints\n",
    "    eval_all_checkpoints=False\n",
    "    #avoid using CUDA when available\n",
    "    no_cuda=True\n",
    "    #overwrite content of output directory\n",
    "    overwrite_output_dir=True\n",
    "    #overwrite cached training and eval sets\n",
    "    overwrite_cache=False\n",
    "    #random seed for initialization\n",
    "    seed=42\n",
    "    #evaluation data set size\n",
    "    dev_data_size=1000\n",
    "    train_data_size=None #LET OP HIER DATA SET KLEINER GEMAAKT\n",
    "\n",
    "    #whether to use 16-bit (mixed) precision instead of 32-bit\n",
    "    fp16=False\n",
    "    #Apex AMP opt level selected ['00', '01', '02', '03']\n",
    "    fp16_opt_level=\"01\"\n",
    "\n",
    "    tp=lambda x:list(x.split(','))\n",
    "    #exclusion list\n",
    "    exclusion_list=[]\n",
    "    #for distributed training\n",
    "    local_rank=-1\n",
    "    #for distant debugging\n",
    "    server_ip=\"\"\n",
    "    #for distant debugging\n",
    "    server_port=\"\"\n",
    "\n",
    "    n_gpu=0\n",
    "\n",
    "args=Args()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "logger=logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES=list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES=tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        block_size=block_size-(tokenizer.model_max_length - tokenizer.max_model_input_sizes)\n",
    "\n",
    "        directory, filename=os.path.split(file_path)\n",
    "        cached_features_file=os.path.join(directory, args.model_type + \"_cached_lm_\"+str(block_size)+\"_\"+filename)\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples=pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "            self.examples=[]\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                text=f.read()\n",
    "\n",
    "            tokenized_text=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "            for i in range(0, len(tokenized_text)-block_size+1, block_size):\n",
    "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dumpt(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, examples: list, labels: list):\n",
    "        self.examples=examples\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.labels:\n",
    "            return torch.tensor(self.examples[i], dtype=torch.long), torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(self.examples[i], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def create_dataset(data, dataset):\n",
    "    d=dict()\n",
    "    for key in data['example'].keys():\n",
    "        if key not in data['label']:\n",
    "            d[key]=dataset(data['example'][key], None)\n",
    "        else:\n",
    "            d[key]=dataset(data['example'][key], data['label'][key])\n",
    "    return d\n",
    "\n",
    "def load_and_cache_examples(data, args, tokenizer):\n",
    "    if args.line_by_line:\n",
    "        train_dataset=create_dataset(data['train'], LineByLineTextDataset)\n",
    "        dev_dataset=create_dataset(data['dev'], LineByLineTextDataset)\n",
    "        return {'train': train_dataset, 'dev': dev_dataset}\n",
    "    #else:\n",
    "     #   return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
    "    #NB; WHAT IS FILE PATH HERE? WILL GIVE ERROR OF LINE_BY_LINE=FALSE\n",
    "\n",
    "def split_data(attributes_examples, attributes_labels, neutral_examples, neutral_labels, args):\n",
    "    data={'train': {'example': {}, 'label': {}}, 'dev': {'example': {}, 'label': {}}}\n",
    "    for i in range(len(attributes_examples)):\n",
    "        for j, (examples, labels) in enumerate(zip(attributes_examples[i], attributes_labels[i])):\n",
    "            idx_l=list(range(len(examples)))\n",
    "            data['train']['example'][f'attribute{i}'] = examples[args.dev_data_size:args.train_data_size]\n",
    "            data['train']['label'][f'attribute{i}'] = labels[args.dev_data_size:args.train_data_size]\n",
    "            data['dev']['example'][f'attribute{i}'] = examples[:args.dev_data_size]\n",
    "            data['dev']['label'][f'attribute{i}'] = labels[:args.dev_data_size]\n",
    "\n",
    "    for i, (neutral_examples, neutral_labels) in enumerate(zip(neutral_examples, neutral_labels)):\n",
    "        idx_l=list(range(len(neutral_examples)))\n",
    "        random.shuffle(idx_l)\n",
    "        neutral_examples=[neutral_examples[idx] for idx in idx_l]\n",
    "        data['train']['example'][f'neutral{i}']=neutral_examples[args.dev_data_size:args.train_data_size]\n",
    "        data['dev']['example'][f'neutral{i}']=neutral_examples[:args.dev_data_size]\n",
    "        if neutral_labels is not None:\n",
    "            neutral_labels=[neutral_labels[idx] for idx in idx_l]\n",
    "            data['train']['label'][f'neutral{i}'] = neutral_labels[args.dev_data_size:args.train_data_size]\n",
    "            data['dev']['label'][f'neutral{i}'] = neutral_labels[:args.dev_data_size]\n",
    "    return data\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "def create_dataloader(args, datasets, tokenizer, train=False):\n",
    "    def collate(batch: List[torch.Tensor]):\n",
    "        if type(batch[0])==tuple:\n",
    "            examples, labels=list(zip(*batch))\n",
    "            return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id), pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "        else:\n",
    "            return pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    dataloaders={}\n",
    "    example_num=0\n",
    "    data_distribution=[]\n",
    "\n",
    "    max_size = max([len(value) for key, value in datasets.items() if key != 'neutral'])\n",
    "    min_size = min([len(value) for key, value in datasets.items() if key != 'neutral'])\n",
    "\n",
    "    for key, dataset in datasets.items():\n",
    "        example_num+=len(dataset)\n",
    "        if train:\n",
    "            #CHECK PER GPU BATCH SIZE VS TRAIN BATCH SIZE\n",
    "            dataloaders[key]=iter(DataLoader(dataset, batch_size=args.per_gpu_train_batch_size, collate_fn=collate, shuffle=True))\n",
    "            data_distribution+=[key for _ in range(int(min_size/args.per_gpu_train_batch_size))]\n",
    "        else:\n",
    "            dataloaders[key]=iter(DataLoader(dataset, batch_size=args.per_gpu_eval_batch_size, collate_fn=collate, shuffle=True))\n",
    "            data_distribution+=[key for _ in range(int(min_size/args.per_gpu_eval_batch_size))]\n",
    "    return dataloaders, example_num, data_distribution\n",
    "\n",
    "def train(args, data,  datasets, model: PreTrainedModel, original_model, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer=SummaryWriter()\n",
    "\n",
    "    args.train_batch_size=args.per_gpu_train_batch_size*max(1, args.n_gpu)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    train_datasets=datasets['train']\n",
    "    dev_datasets=datasets['dev']\n",
    "\n",
    "    train_dataloaders, train_example_num, train_distribution=create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "\n",
    "    train_iter_num=sum([len(dataloader) for dataloader in train_dataloaders.values()])\n",
    "    dev_iter_num=sum([len(dataloader) for dataloader in dev_dataloaders.values()])\n",
    "\n",
    "    if args.max_steps>0:\n",
    "        t_total=args.max_steps\n",
    "        args.num_train_epochs=args.max_steps//(train_iter_num//args.gradient_accumulation_steps)+1\n",
    "    else:\n",
    "        t_total = train_iter_num // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model=model.module if hasattr(model, \"module\") else model\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    original_model = original_model.module if hasattr(original_model, \"module\") else original_model  # Take care of distributed/parallel training\n",
    "    original_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters=[\n",
    "        {\n",
    "            \"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        original_model = torch.nn.DataParallel(original_model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n",
    "        original_model = torch.nn.parallel.DistributedDataParallel(\n",
    "            original_model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n",
    "\n",
    "    #Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", train_example_num)\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step=0\n",
    "    epochs_trained=0\n",
    "    best_loss=float('inf')\n",
    "    best_step=0\n",
    "    steps_trained_in_current_epoch=0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (train_iter_num // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (train_iter_num // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    model.zero_grad()\n",
    "    original_model.zero_grad()\n",
    "    #train_iterator=trange(epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    train_iterator=range(epochs_trained, int(args.num_train_epochs))\n",
    "\n",
    "    def inner_product(x,y):\n",
    "        return torch.mean(torch.sum(y*x, 3))\n",
    "\n",
    "    def mean_square(x,y,idx):\n",
    "        return torch.mean(torch.mean((y-x)**2, idx))\n",
    "\n",
    "    def save_best_model(best_loss, best_step, dev_dataloaders):\n",
    "        if (args.local_rank == -1 and args.evaluate_during_training):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "            eval_loss = evaluate(model, attributes_hiddens, dev_dataloaders)\n",
    "            logger.info(\" global_step = %s, evaluate loss = %s\", global_step, eval_loss)\n",
    "            tb_writer.add_scalar(\"eval_loss\", eval_loss, global_step)\n",
    "        tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "\n",
    "        if eval_loss<best_loss:\n",
    "            best_loss = eval_loss\n",
    "            best_step = global_step\n",
    "            checkpoint_prefix = \"checkpoint\"\n",
    "            # Save model checkpoint\n",
    "            output_dir = os.path.join(args.output_dir, \"debiased-checkpoint-best\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        logger.info(\" best_step = %s, best loss = %s\", best_step, best_loss)\n",
    "\n",
    "        original_output_dir=os.path.join(args.output_dir, \"original-checkpoint-best\")\n",
    "        os.makedirs(original_output_dir, exist_ok=True)\n",
    "        original_model_to_save=(\n",
    "            original_model.module if hasattr(original_model, \"module\") else original_model\n",
    "        )\n",
    "        original_model_to_save.save_pretrained(original_output_dir)\n",
    "        tokenizer.save_pretrained(original_output_dir)\n",
    "\n",
    "        torch.save(args, os.path.join(original_output_dir, \"training_args.bin\"))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(original_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(original_output_dir, \"scheduler.pt\"))\n",
    "        return best_loss, best_step\n",
    "\n",
    "    def get_hiddens_of_model(input):\n",
    "        model.zero_grad()\n",
    "        if args.model_type == 'roberta':\n",
    "            #_, _, hiddens = model.roberta(input)\n",
    "            hiddens=model.roberta(input).hidden_states\n",
    "        elif args.model_type == 'bert':\n",
    "            #changed\n",
    "            hiddens=model.bert(input).hidden_states\n",
    "        elif args.model_type == 'albert':\n",
    "            #_, _, hiddens = model.albert(input)\n",
    "            hiddens=model.albert(input).hidden_states\n",
    "        elif args.model_type == 'dbert':\n",
    "            #changed\n",
    "            #_, hiddens = model.distilbert(input)\n",
    "            hiddens=model.distilbert(input).hidden_states\n",
    "        elif args.model_type == 'electra':\n",
    "            _, hiddens = model.electra(input)\n",
    "        elif args.model_type == 'gpt2':\n",
    "            #_, _, hiddens = model.transformer(input)\n",
    "            hiddens=model.transformer(input).hidden_states\n",
    "        elif args.model_type == 'gpt':\n",
    "            _, hiddens = model.transformer(input)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    def attribute_vector_example():\n",
    "        attributes_hiddens = {f'attribute{i}': [] for i in range(len(args.data_files))}\n",
    "\n",
    "        dataloaders, _, distribution = create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "        for key in distribution:\n",
    "            if 'neutral' not in key:\n",
    "                inputs, labels = next(dataloaders[key])\n",
    "                inputs = inputs.to(args.device)\n",
    "                hiddens = get_hiddens_of_model(inputs)\n",
    "                hiddens = torch.stack(hiddens, 2)\n",
    "                if labels.size(1) > 1:\n",
    "                    onehot = torch.eye(hiddens.size(1))\n",
    "                    zeros = torch.zeros(1, onehot.size(0))\n",
    "                    onehot = torch.cat((zeros, onehot), 0)\n",
    "                    onehot = onehot[labels]\n",
    "                    onehot = torch.sum(onehot, 1)\n",
    "                    onehot = onehot.view(hiddens.size(0), -1, 1, 1)\n",
    "                else:\n",
    "                    onehot = torch.eye(hiddens.size(1))[labels].view(hiddens.size(0), -1, 1, 1)\n",
    "                onehot = onehot.to(args.device)\n",
    "                attributes_hiddens[key].append(torch.sum(hiddens * onehot, 1) / labels.size(1))\n",
    "\n",
    "        # neutral\n",
    "        attribute_size = len(args.data_files)\n",
    "        for i in range(attribute_size):\n",
    "            attributes_hiddens[f'attribute{i}'] = torch.mean(torch.cat(attributes_hiddens[f'attribute{i}'], 0), 0).detach().unsqueeze(0)\n",
    "\n",
    "        return attributes_hiddens\n",
    "\n",
    "    def forward(attributes_hiddens, dataloaders, key):\n",
    "        inputs = next(dataloaders[key])\n",
    "        if len(inputs) == 2:\n",
    "            inputs, labels = inputs\n",
    "            labels = labels.to(args.device)\n",
    "        else:\n",
    "            labels = None\n",
    "        inputs = inputs.to(args.device)\n",
    "        if args.model_type == 'roberta':\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.roberta(inputs)\n",
    "            all_layer_hiddens=model.roberta(inputs).hidden_states\n",
    "            final_layer_hiddens=model.roberta(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.roberta(inputs)\n",
    "                    all_layer_original_hiddens=original_model.roberta(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.roberta(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.lm_head(final_layer_hiddens)\n",
    "                    token_original = original_model.lm_head(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'bert':\n",
    "            all_layer_hiddens=model.bert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.bert(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    all_layer_original_hiddens=original_model.bert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.bert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.cls(final_layer_hiddens)\n",
    "                    token_original = original_model.cls(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'albert':\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.albert(inputs)\n",
    "            all_layer_hiddens=model.albert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.albert(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.albert(inputs)\n",
    "                    all_layer_original_hiddens=original_model.albert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.albert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.classifier(final_layer_hiddens)\n",
    "                    token_original = original_model.classifier(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'dbert':\n",
    "            #final_layer_hiddens, all_layer_hiddens = model.distilbert(inputs)\n",
    "            all_layer_hiddens=model.distilbert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.distilbert(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, all_layer_original_hiddens = original_model.distilbert(inputs)\n",
    "                    all_layer_original_hiddens=original_model.distilbert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.distilbert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.classifier(final_layer_hiddens)\n",
    "                    token_original = original_model.classifier(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'electra':\n",
    "            final_layer_hiddens, all_layer_hiddens = model.electra(inputs)\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    final_layer_original_hiddens, all_layer_original_hiddens = original_model.electra(inputs)\n",
    "                if args.token_loss:\n",
    "                    hiddens = model.generator_predictions(final_layer_hiddens)\n",
    "                    token_predicts = model.generator_lm_head(hiddens)\n",
    "                    original_hiddens = original_model.generator_predictions(final_layer_original_hiddens)\n",
    "                    token_original = original_model.generator_lm_head(original_hiddens)\n",
    "        elif args.model_type == 'gpt2':\n",
    "            all_layer_hiddens=model.transformer(inputs).hidden_states\n",
    "            final_layer_hiddens=model.transformer(inputs).last_hidden_state\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.transformer(inputs)\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                   #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.transformer(inputs)\n",
    "                    all_layer_original_hiddens=original_model.transformer(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.transformer(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.lm_head(final_layer_hiddens)\n",
    "                    token_original = original_model.lm_head(final_layer_original_hiddens)\n",
    "\n",
    "        all_layer_hiddens = torch.stack(all_layer_hiddens, 2)\n",
    "        if 'neutral' not in key:\n",
    "            all_original_hiddens =  torch.stack(all_layer_original_hiddens, 2)\n",
    "            all_original_hiddens = all_original_hiddens.detach()\n",
    "            if args.token_loss:\n",
    "                original_hiddens - original_hiddens.detach()\n",
    "                token_original = token_original.detach()\n",
    "        if args.debias_layer == 'all':\n",
    "            target_layer_hiddens = all_layer_hiddens\n",
    "            target_original_hiddens = all_layer_hiddens\n",
    "        else:\n",
    "            if args.debias_layer == 'first':\n",
    "                idx = 0\n",
    "            elif args.debias_layer == 'last':\n",
    "                idx = -1\n",
    "            target_layer_hiddens = all_layer_hiddens[:,:,idx]\n",
    "            target_layer_hiddens = target_layer_hiddens.unsqueeze(2)\n",
    "            if 'neutral' not in key:\n",
    "                target_original_hiddens = all_original_hiddens[:,:,idx]\n",
    "                target_original_hiddens = target_original_hiddens.unsqueeze(2)\n",
    "            else:\n",
    "                #attributes_hiddens = {key: value[:,idx,:].unsqueeze(2) for key, value in attributes_hiddens.items()}\n",
    "                attributes_hiddens = {key: value[:,idx,:].unsqueeze(1) for key, value in attributes_hiddens.items()}\n",
    "                #KLOPT DE UNSQUEEZE DIMENSIE HIER?\n",
    "\n",
    "        if args.loss_target == 'sentence' or labels is None:\n",
    "            attributes_hiddens = {key: value.unsqueeze(1) for key, value in attributes_hiddens.items()}\n",
    "        #elif args.loss_target == 'token' and key == 'neutral':\n",
    "        elif args.loss_target == 'token':\n",
    "            if labels.size(1) > 1:\n",
    "                onehot = torch.eye(target_layer_hiddens.size(1))\n",
    "                zeros = torch.zeros(1, onehot.size(0))\n",
    "                onehot = torch.cat((zeros, onehot), 0)\n",
    "                onehot = onehot[labels]\n",
    "                onehot = torch.sum(onehot, 1)\n",
    "                onehot = onehot.view(target_layer_hiddens.size(0), -1, 1, 1)\n",
    "            else:\n",
    "                onehot = torch.eye(target_layer_hiddens.size(1))[labels].view(target_layer_hiddens.size(0), -1, 1, 1)\n",
    "            onehot = onehot.to(args.device)\n",
    "            target_layer_hiddens = torch.sum(target_layer_hiddens * onehot, 1).unsqueeze(1) / labels.size(1)\n",
    "            if 'neutral' not in key:\n",
    "                target_original_hiddens = torch.sum(target_original_hiddens * onehot, 1).unsqueeze(1) / labels.size(1)\n",
    "                #KLOPT UNSQUEEZE HIER?\n",
    "            else:\n",
    "                attributes_hiddens = {key: value.expand(target_layer_hiddens.size(0), 1, value.size(1),value.size(2)) for key, value in attributes_hiddens.items()}\n",
    "\n",
    "        if 'neutral' in key: #VERANDER ALS MEER CLASSES\n",
    "            loss = 0\n",
    "            if key=='neutral0':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute0'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "            elif key=='neutral1':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute1'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "            elif key=='neutral2':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute2'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "            elif key=='neutral3':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute3'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "        else:\n",
    "            #loss = criterion_ms(target_layer_hiddens, target_original_hiddens)\n",
    "            loss = criterion_ms(all_layer_hiddens, all_original_hiddens, 3)\n",
    "            if args.token_loss:\n",
    "                loss += criterion_ms(token_predicts, token_original, 2)\n",
    "                #loss += criterion_ms(hiddens, original_hiddens, 2)\n",
    "            loss *= beta\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(model, attributes_hiddens, dev_dataloaders, prefix=\"\"):\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        eval_output_dir = args.output_dir\n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "        args.eval_batch_size=args.per_gpu_eval_batch_size*max(1,args.n_gpu)\n",
    "        # multi-gpu evaluate\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", dev_example_num)\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        for key in tqdm(dev_distribution):\n",
    "            with torch.no_grad():\n",
    "                loss = forward(attributes_hiddens, dev_dataloaders, key)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "            logger.info(\"  Loss = %s\", eval_loss)\n",
    "            writer.write(\"Loss = %s\\n\" % (eval_loss))\n",
    "\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    criterion_ms=mean_square\n",
    "    criterion_ip=inner_product\n",
    "    original_model.eval()\n",
    "\n",
    "    alpha, beta=args.weighted_loss\n",
    "    alpha=float(alpha)\n",
    "    beta=float(beta)\n",
    "\n",
    "    train_loss=0.0\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        random.shuffle(train_distribution)\n",
    "        epoch_iterator = tqdm(train_distribution, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            attributes_hiddens = attribute_vector_example()\n",
    "\n",
    "        for step, key in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            loss = forward(attributes_hiddens, train_dataloaders, key)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    logger.info(\" global_step = %s, train loss = %s\", global_step, train_loss)\n",
    "                    train_loss = 0.0\n",
    "                    # Log metrics\n",
    "                    best_loss, best_step = save_best_model(best_loss, best_step, dev_dataloaders)\n",
    "                    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "            train_dataloaders, train_example_num, train_distribution = create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "    best_loss, best_step = save_best_model(best_loss, best_step, dev_dataloaders)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/08/2023 22:02:02 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "02/08/2023 22:02:15 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x0000029DB26F0AC0>\n",
      "02/08/2023 22:02:20 - INFO - __main__ -   ***** Running training *****\n",
      "02/08/2023 22:02:20 - INFO - __main__ -     Num examples = 178846\n",
      "02/08/2023 22:02:20 - INFO - __main__ -     Num Epochs = 2\n",
      "02/08/2023 22:02:20 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
      "02/08/2023 22:02:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "02/08/2023 22:02:20 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/08/2023 22:02:20 - INFO - __main__ -     Total optimization steps = 11184\n",
      "Iteration:  79%|███████▉  | 499/632 [2:23:49<50:54, 22.97s/it]  02/09/2023 00:26:28 - INFO - __main__ -    global_step = 500, train loss = 1887308861.916757\n",
      "02/09/2023 00:26:28 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "02/09/2023 00:26:28 - INFO - __main__ -     Num examples = 8000\n",
      "02/09/2023 00:26:28 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "  0%|          | 0/248 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/248 [00:11<46:48, 11.37s/it]\u001B[A\n",
      "  1%|          | 2/248 [00:26<55:43, 13.59s/it]\u001B[A\n",
      "  1%|          | 3/248 [00:36<48:03, 11.77s/it]\u001B[A\n",
      "  2%|▏         | 4/248 [00:47<47:25, 11.66s/it]\u001B[A\n",
      "  2%|▏         | 5/248 [00:58<45:57, 11.35s/it]\u001B[A\n",
      "  2%|▏         | 6/248 [01:09<46:04, 11.42s/it]\u001B[A\n",
      "  3%|▎         | 7/248 [01:23<48:45, 12.14s/it]\u001B[A\n",
      "  3%|▎         | 8/248 [01:32<44:30, 11.13s/it]\u001B[A\n",
      "  4%|▎         | 9/248 [01:41<41:34, 10.44s/it]\u001B[A\n",
      "  4%|▍         | 10/248 [01:52<41:50, 10.55s/it]\u001B[A\n",
      "  4%|▍         | 11/248 [02:06<46:24, 11.75s/it]\u001B[A\n",
      "  5%|▍         | 12/248 [02:15<42:31, 10.81s/it]\u001B[A\n",
      "  5%|▌         | 13/248 [02:25<41:42, 10.65s/it]\u001B[A\n",
      "  6%|▌         | 14/248 [02:37<42:32, 10.91s/it]\u001B[A\n",
      "  6%|▌         | 15/248 [02:46<40:51, 10.52s/it]\u001B[A\n",
      "  6%|▋         | 16/248 [02:56<39:49, 10.30s/it]\u001B[A\n",
      "  7%|▋         | 17/248 [03:06<39:42, 10.32s/it]\u001B[A\n",
      "  7%|▋         | 18/248 [03:20<43:17, 11.29s/it]\u001B[A\n",
      "  8%|▊         | 19/248 [03:30<41:17, 10.82s/it]\u001B[A\n",
      "  8%|▊         | 20/248 [03:41<41:50, 11.01s/it]\u001B[A\n",
      "  8%|▊         | 21/248 [04:04<54:33, 14.42s/it]\u001B[A\n",
      "  9%|▉         | 22/248 [04:15<51:02, 13.55s/it]\u001B[A\n",
      "  9%|▉         | 23/248 [04:28<50:27, 13.46s/it]\u001B[A\n",
      " 10%|▉         | 24/248 [04:46<55:06, 14.76s/it]\u001B[A\n",
      " 10%|█         | 25/248 [04:58<51:21, 13.82s/it]\u001B[A\n",
      " 10%|█         | 26/248 [05:08<46:57, 12.69s/it]\u001B[A\n",
      " 11%|█         | 27/248 [05:18<43:55, 11.92s/it]\u001B[A\n",
      " 11%|█▏        | 28/248 [05:30<43:44, 11.93s/it]\u001B[A\n",
      " 12%|█▏        | 29/248 [05:40<41:12, 11.29s/it]\u001B[A\n",
      " 12%|█▏        | 30/248 [05:50<39:58, 11.00s/it]\u001B[A\n",
      " 12%|█▎        | 31/248 [06:10<49:16, 13.62s/it]\u001B[A\n",
      " 13%|█▎        | 32/248 [06:32<58:26, 16.24s/it]\u001B[A\n",
      " 13%|█▎        | 33/248 [06:44<53:07, 14.82s/it]\u001B[A\n",
      " 14%|█▎        | 34/248 [06:58<52:07, 14.62s/it]\u001B[A\n",
      " 14%|█▍        | 35/248 [07:15<55:06, 15.52s/it]\u001B[A\n",
      " 15%|█▍        | 36/248 [07:27<50:39, 14.33s/it]\u001B[A\n",
      " 15%|█▍        | 37/248 [07:38<47:25, 13.48s/it]\u001B[A\n",
      " 15%|█▌        | 38/248 [07:50<45:08, 12.90s/it]\u001B[A\n",
      " 16%|█▌        | 39/248 [08:04<46:27, 13.34s/it]\u001B[A\n",
      " 16%|█▌        | 40/248 [08:17<45:39, 13.17s/it]\u001B[A\n",
      " 17%|█▋        | 41/248 [08:32<46:51, 13.58s/it]\u001B[A\n",
      " 17%|█▋        | 42/248 [08:42<43:27, 12.66s/it]\u001B[A\n",
      " 17%|█▋        | 43/248 [08:53<41:53, 12.26s/it]\u001B[A\n",
      " 18%|█▊        | 44/248 [09:04<39:44, 11.69s/it]\u001B[A\n",
      " 18%|█▊        | 45/248 [09:16<39:53, 11.79s/it]\u001B[A\n",
      " 19%|█▊        | 46/248 [09:26<38:03, 11.30s/it]\u001B[A\n",
      " 19%|█▉        | 47/248 [09:37<37:16, 11.13s/it]\u001B[A\n",
      " 19%|█▉        | 48/248 [09:54<42:56, 12.88s/it]\u001B[A\n",
      " 20%|█▉        | 49/248 [10:07<43:01, 12.97s/it]\u001B[A\n",
      " 20%|██        | 50/248 [10:17<39:57, 12.11s/it]\u001B[A\n",
      " 21%|██        | 51/248 [10:29<39:23, 12.00s/it]\u001B[A\n",
      " 21%|██        | 52/248 [10:39<37:05, 11.35s/it]\u001B[A\n",
      " 21%|██▏       | 53/248 [10:50<37:05, 11.42s/it]\u001B[A\n",
      " 22%|██▏       | 54/248 [11:01<36:36, 11.32s/it]\u001B[A\n",
      " 22%|██▏       | 55/248 [11:13<36:51, 11.46s/it]\u001B[A\n",
      " 23%|██▎       | 56/248 [11:22<34:36, 10.82s/it]\u001B[A\n",
      " 23%|██▎       | 57/248 [11:34<35:06, 11.03s/it]\u001B[A\n",
      " 23%|██▎       | 58/248 [11:44<34:14, 10.81s/it]\u001B[A\n",
      " 24%|██▍       | 59/248 [11:58<36:34, 11.61s/it]\u001B[A\n",
      " 24%|██▍       | 60/248 [12:09<35:54, 11.46s/it]\u001B[A\n",
      " 25%|██▍       | 61/248 [12:20<35:25, 11.37s/it]\u001B[A\n",
      " 25%|██▌       | 62/248 [12:30<34:18, 11.07s/it]\u001B[A\n",
      " 25%|██▌       | 63/248 [12:41<33:36, 10.90s/it]\u001B[A\n",
      " 26%|██▌       | 64/248 [12:54<35:09, 11.47s/it]\u001B[A\n",
      " 26%|██▌       | 65/248 [13:05<34:50, 11.42s/it]\u001B[A\n",
      " 27%|██▋       | 66/248 [13:14<32:53, 10.84s/it]\u001B[A\n",
      " 27%|██▋       | 67/248 [13:26<33:44, 11.19s/it]\u001B[A\n",
      " 27%|██▋       | 68/248 [13:38<33:42, 11.24s/it]\u001B[A\n",
      " 28%|██▊       | 69/248 [13:49<33:50, 11.34s/it]\u001B[A\n",
      " 28%|██▊       | 70/248 [13:59<32:17, 10.88s/it]\u001B[A\n",
      " 29%|██▊       | 71/248 [14:13<35:07, 11.91s/it]\u001B[A\n",
      " 29%|██▉       | 72/248 [14:25<34:17, 11.69s/it]\u001B[A\n",
      " 29%|██▉       | 73/248 [14:35<32:57, 11.30s/it]\u001B[A\n",
      " 30%|██▉       | 74/248 [14:46<32:16, 11.13s/it]\u001B[A\n",
      " 30%|███       | 75/248 [14:59<33:51, 11.74s/it]\u001B[A\n",
      " 31%|███       | 76/248 [15:21<42:34, 14.85s/it]\u001B[A\n",
      " 31%|███       | 77/248 [15:35<41:35, 14.59s/it]\u001B[A\n",
      " 31%|███▏      | 78/248 [15:47<39:10, 13.83s/it]\u001B[A\n",
      " 32%|███▏      | 79/248 [15:56<35:02, 12.44s/it]\u001B[A\n",
      " 32%|███▏      | 80/248 [16:07<33:45, 12.05s/it]\u001B[A\n",
      " 33%|███▎      | 81/248 [16:18<32:06, 11.53s/it]\u001B[A\n",
      " 33%|███▎      | 82/248 [16:29<31:51, 11.51s/it]\u001B[A\n",
      " 33%|███▎      | 83/248 [16:41<31:45, 11.55s/it]\u001B[A\n",
      " 34%|███▍      | 84/248 [16:53<31:58, 11.70s/it]\u001B[A\n",
      " 34%|███▍      | 85/248 [17:05<32:09, 11.84s/it]\u001B[A\n",
      " 35%|███▍      | 86/248 [17:17<31:59, 11.85s/it]\u001B[A\n",
      " 35%|███▌      | 87/248 [17:34<35:54, 13.38s/it]\u001B[A\n",
      " 35%|███▌      | 88/248 [17:48<36:21, 13.63s/it]\u001B[A\n",
      " 36%|███▌      | 89/248 [18:06<39:13, 14.80s/it]\u001B[A\n",
      " 36%|███▋      | 90/248 [18:19<37:54, 14.39s/it]\u001B[A\n",
      " 37%|███▋      | 91/248 [18:31<35:36, 13.61s/it]\u001B[A\n",
      " 37%|███▋      | 92/248 [18:42<33:09, 12.75s/it]\u001B[A\n",
      " 38%|███▊      | 93/248 [18:53<31:55, 12.36s/it]\u001B[A\n",
      " 38%|███▊      | 94/248 [19:04<30:36, 11.92s/it]\u001B[A\n",
      " 38%|███▊      | 95/248 [19:15<29:51, 11.71s/it]\u001B[A\n",
      " 39%|███▊      | 96/248 [19:27<29:25, 11.62s/it]\u001B[A\n",
      " 39%|███▉      | 97/248 [19:38<28:44, 11.42s/it]\u001B[A\n",
      " 40%|███▉      | 98/248 [19:53<31:44, 12.70s/it]\u001B[A\n",
      " 40%|███▉      | 99/248 [20:11<35:01, 14.10s/it]\u001B[A\n",
      " 40%|████      | 100/248 [20:23<33:48, 13.71s/it]\u001B[A\n",
      " 41%|████      | 101/248 [20:34<31:18, 12.78s/it]\u001B[A\n",
      " 41%|████      | 102/248 [20:44<29:19, 12.05s/it]\u001B[A\n",
      " 42%|████▏     | 103/248 [20:57<29:32, 12.23s/it]\u001B[A\n",
      " 42%|████▏     | 104/248 [21:13<32:10, 13.41s/it]\u001B[A\n",
      " 42%|████▏     | 105/248 [21:25<31:02, 13.02s/it]\u001B[A\n",
      " 43%|████▎     | 106/248 [21:39<31:15, 13.21s/it]\u001B[A\n",
      " 43%|████▎     | 107/248 [21:51<30:14, 12.87s/it]\u001B[A\n",
      " 44%|████▎     | 108/248 [22:07<32:32, 13.94s/it]\u001B[A\n",
      " 44%|████▍     | 109/248 [22:19<30:21, 13.10s/it]\u001B[A\n",
      " 44%|████▍     | 110/248 [22:31<29:38, 12.89s/it]\u001B[A\n",
      " 45%|████▍     | 111/248 [22:42<27:53, 12.21s/it]\u001B[A\n",
      " 45%|████▌     | 112/248 [22:53<26:50, 11.84s/it]\u001B[A\n",
      " 46%|████▌     | 113/248 [23:06<28:00, 12.45s/it]\u001B[A\n",
      " 46%|████▌     | 114/248 [23:18<27:29, 12.31s/it]\u001B[A\n",
      " 46%|████▋     | 115/248 [23:41<34:01, 15.35s/it]\u001B[A\n",
      " 47%|████▋     | 116/248 [23:51<30:37, 13.92s/it]\u001B[A\n",
      " 47%|████▋     | 117/248 [24:02<28:15, 12.94s/it]\u001B[A\n",
      " 48%|████▊     | 118/248 [24:17<29:16, 13.51s/it]\u001B[A\n",
      " 48%|████▊     | 119/248 [24:32<30:05, 13.99s/it]\u001B[A\n",
      " 48%|████▊     | 120/248 [24:44<28:29, 13.35s/it]\u001B[A\n",
      " 49%|████▉     | 121/248 [24:56<27:13, 12.86s/it]\u001B[A\n",
      " 49%|████▉     | 122/248 [25:07<26:11, 12.47s/it]\u001B[A\n",
      " 50%|████▉     | 123/248 [25:22<27:16, 13.09s/it]\u001B[A\n",
      " 50%|█████     | 124/248 [25:32<25:25, 12.30s/it]\u001B[A\n",
      " 50%|█████     | 125/248 [25:38<21:09, 10.32s/it]\u001B[A\n",
      " 51%|█████     | 126/248 [25:43<17:43,  8.72s/it]\u001B[A\n",
      " 51%|█████     | 127/248 [25:50<16:21,  8.11s/it]\u001B[A\n",
      " 52%|█████▏    | 128/248 [25:56<15:13,  7.61s/it]\u001B[A\n",
      " 52%|█████▏    | 129/248 [26:02<14:07,  7.12s/it]\u001B[A\n",
      " 52%|█████▏    | 130/248 [26:07<12:35,  6.40s/it]\u001B[A\n",
      " 53%|█████▎    | 131/248 [26:13<12:31,  6.42s/it]\u001B[A\n",
      " 53%|█████▎    | 132/248 [26:24<14:39,  7.58s/it]\u001B[A\n",
      " 54%|█████▎    | 133/248 [26:29<13:29,  7.04s/it]\u001B[A\n",
      " 54%|█████▍    | 134/248 [26:34<12:10,  6.41s/it]\u001B[A\n",
      " 54%|█████▍    | 135/248 [26:41<12:07,  6.44s/it]\u001B[A\n",
      " 55%|█████▍    | 136/248 [26:47<11:42,  6.27s/it]\u001B[A\n",
      " 55%|█████▌    | 137/248 [26:52<11:01,  5.96s/it]\u001B[A\n",
      " 56%|█████▌    | 138/248 [26:58<10:48,  5.89s/it]\u001B[A\n",
      " 56%|█████▌    | 139/248 [27:03<10:33,  5.81s/it]\u001B[A\n",
      " 56%|█████▋    | 140/248 [27:11<11:34,  6.43s/it]\u001B[A\n",
      " 57%|█████▋    | 141/248 [27:17<10:58,  6.15s/it]\u001B[A\n",
      " 57%|█████▋    | 142/248 [27:22<10:33,  5.98s/it]\u001B[A\n",
      " 58%|█████▊    | 143/248 [27:30<11:35,  6.62s/it]\u001B[A\n",
      " 58%|█████▊    | 144/248 [27:35<10:36,  6.12s/it]\u001B[A\n",
      " 58%|█████▊    | 145/248 [27:41<10:22,  6.04s/it]\u001B[A\n",
      " 59%|█████▉    | 146/248 [27:46<09:52,  5.81s/it]\u001B[A\n",
      " 59%|█████▉    | 147/248 [27:51<09:06,  5.41s/it]\u001B[A\n",
      " 60%|█████▉    | 148/248 [27:55<08:35,  5.15s/it]\u001B[A\n",
      " 60%|██████    | 149/248 [28:01<08:37,  5.22s/it]\u001B[A\n",
      " 60%|██████    | 150/248 [28:06<08:29,  5.20s/it]\u001B[A\n",
      " 61%|██████    | 151/248 [28:12<08:37,  5.34s/it]\u001B[A\n",
      " 61%|██████▏   | 152/248 [28:18<09:05,  5.68s/it]\u001B[A\n",
      " 62%|██████▏   | 153/248 [28:23<08:44,  5.52s/it]\u001B[A\n",
      " 62%|██████▏   | 154/248 [28:30<09:07,  5.83s/it]\u001B[A\n",
      " 62%|██████▎   | 155/248 [28:38<10:14,  6.61s/it]\u001B[A\n",
      " 63%|██████▎   | 156/248 [28:45<10:04,  6.57s/it]\u001B[A\n",
      " 63%|██████▎   | 157/248 [28:51<10:00,  6.60s/it]\u001B[A\n",
      " 64%|██████▎   | 158/248 [28:56<09:15,  6.18s/it]\u001B[A\n",
      " 64%|██████▍   | 159/248 [29:02<08:45,  5.90s/it]\u001B[A\n",
      " 65%|██████▍   | 160/248 [29:06<07:58,  5.43s/it]\u001B[A\n",
      " 65%|██████▍   | 161/248 [29:10<07:18,  5.04s/it]\u001B[A\n",
      " 65%|██████▌   | 162/248 [29:15<07:14,  5.06s/it]\u001B[A\n",
      " 66%|██████▌   | 163/248 [29:21<07:18,  5.16s/it]\u001B[A\n",
      " 66%|██████▌   | 164/248 [29:26<07:13,  5.16s/it]\u001B[A\n",
      " 67%|██████▋   | 165/248 [29:31<07:04,  5.11s/it]\u001B[A\n",
      " 67%|██████▋   | 166/248 [29:36<07:03,  5.16s/it]\u001B[A\n",
      " 67%|██████▋   | 167/248 [29:42<07:18,  5.41s/it]\u001B[A\n",
      " 68%|██████▊   | 168/248 [29:46<06:40,  5.01s/it]\u001B[A\n",
      " 68%|██████▊   | 169/248 [29:51<06:36,  5.02s/it]\u001B[A\n",
      " 69%|██████▊   | 170/248 [29:57<06:40,  5.13s/it]\u001B[A\n",
      " 69%|██████▉   | 171/248 [30:02<06:46,  5.28s/it]\u001B[A\n",
      " 69%|██████▉   | 172/248 [30:08<06:54,  5.45s/it]\u001B[A\n",
      " 70%|██████▉   | 173/248 [30:13<06:27,  5.17s/it]\u001B[A\n",
      " 70%|███████   | 174/248 [30:18<06:36,  5.36s/it]\u001B[A\n",
      " 71%|███████   | 175/248 [30:23<06:23,  5.25s/it]\u001B[A\n",
      " 71%|███████   | 176/248 [30:30<06:54,  5.75s/it]\u001B[A\n",
      " 71%|███████▏  | 177/248 [30:37<06:57,  5.88s/it]\u001B[A\n",
      " 72%|███████▏  | 178/248 [30:41<06:22,  5.46s/it]\u001B[A\n",
      " 72%|███████▏  | 179/248 [30:46<06:07,  5.32s/it]\u001B[A\n",
      " 73%|███████▎  | 180/248 [30:54<06:59,  6.18s/it]\u001B[A\n",
      " 73%|███████▎  | 181/248 [31:04<08:01,  7.18s/it]\u001B[A\n",
      " 73%|███████▎  | 182/248 [31:09<07:14,  6.58s/it]\u001B[A\n",
      " 74%|███████▍  | 183/248 [31:15<06:53,  6.35s/it]\u001B[A\n",
      " 74%|███████▍  | 184/248 [31:20<06:24,  6.01s/it]\u001B[A\n",
      " 75%|███████▍  | 185/248 [31:25<06:01,  5.74s/it]\u001B[A\n",
      " 75%|███████▌  | 186/248 [31:31<05:56,  5.75s/it]\u001B[A\n",
      " 75%|███████▌  | 187/248 [31:41<07:18,  7.19s/it]\u001B[A\n",
      " 76%|███████▌  | 188/248 [31:47<06:40,  6.67s/it]\u001B[A\n",
      " 76%|███████▌  | 189/248 [31:52<06:03,  6.16s/it]\u001B[A\n",
      " 77%|███████▋  | 190/248 [31:57<05:41,  5.89s/it]\u001B[A\n",
      " 77%|███████▋  | 191/248 [32:03<05:31,  5.81s/it]\u001B[A\n",
      " 77%|███████▋  | 192/248 [32:08<05:09,  5.53s/it]\u001B[A\n",
      " 78%|███████▊  | 193/248 [32:12<04:49,  5.27s/it]\u001B[A\n",
      " 78%|███████▊  | 194/248 [32:18<04:47,  5.32s/it]\u001B[A\n",
      " 79%|███████▊  | 195/248 [32:22<04:34,  5.18s/it]\u001B[A\n",
      " 79%|███████▉  | 196/248 [32:29<04:43,  5.46s/it]\u001B[A\n",
      " 79%|███████▉  | 197/248 [32:34<04:32,  5.34s/it]\u001B[A\n",
      " 80%|███████▉  | 198/248 [32:40<04:35,  5.50s/it]\u001B[A\n",
      " 80%|████████  | 199/248 [32:45<04:27,  5.47s/it]\u001B[A\n",
      " 81%|████████  | 200/248 [32:50<04:12,  5.26s/it]\u001B[A\n",
      " 81%|████████  | 201/248 [32:58<04:43,  6.03s/it]\u001B[A\n",
      " 81%|████████▏ | 202/248 [33:05<04:54,  6.41s/it]\u001B[A\n",
      " 82%|████████▏ | 203/248 [33:10<04:28,  5.97s/it]\u001B[A\n",
      " 82%|████████▏ | 204/248 [33:16<04:28,  6.10s/it]\u001B[A\n",
      " 83%|████████▎ | 205/248 [33:23<04:25,  6.18s/it]\u001B[A\n",
      " 83%|████████▎ | 206/248 [33:29<04:23,  6.28s/it]\u001B[A\n",
      " 83%|████████▎ | 207/248 [33:34<04:05,  5.98s/it]\u001B[A\n",
      " 84%|████████▍ | 208/248 [33:38<03:37,  5.43s/it]\u001B[A\n",
      " 84%|████████▍ | 209/248 [33:44<03:33,  5.47s/it]\u001B[A\n",
      " 85%|████████▍ | 210/248 [33:50<03:38,  5.76s/it]\u001B[A\n",
      " 85%|████████▌ | 211/248 [33:57<03:37,  5.87s/it]\u001B[A\n",
      " 85%|████████▌ | 212/248 [34:03<03:42,  6.18s/it]\u001B[A\n",
      " 86%|████████▌ | 213/248 [34:08<03:24,  5.83s/it]\u001B[A\n",
      " 86%|████████▋ | 214/248 [34:14<03:18,  5.84s/it]\u001B[A\n",
      " 87%|████████▋ | 215/248 [34:20<03:08,  5.72s/it]\u001B[A\n",
      " 87%|████████▋ | 216/248 [34:25<02:54,  5.44s/it]\u001B[A\n",
      " 88%|████████▊ | 217/248 [34:31<02:53,  5.60s/it]\u001B[A\n",
      " 88%|████████▊ | 218/248 [34:36<02:43,  5.46s/it]\u001B[A\n",
      " 88%|████████▊ | 219/248 [34:41<02:34,  5.33s/it]\u001B[A\n",
      " 89%|████████▊ | 220/248 [34:46<02:27,  5.28s/it]\u001B[A\n",
      " 89%|████████▉ | 221/248 [34:51<02:22,  5.26s/it]\u001B[A\n",
      " 90%|████████▉ | 222/248 [34:56<02:16,  5.26s/it]\u001B[A\n",
      " 90%|████████▉ | 223/248 [35:02<02:11,  5.26s/it]\u001B[A\n",
      " 90%|█████████ | 224/248 [35:07<02:10,  5.45s/it]\u001B[A\n",
      " 91%|█████████ | 225/248 [35:17<02:30,  6.53s/it]\u001B[A\n",
      " 91%|█████████ | 226/248 [35:22<02:13,  6.07s/it]\u001B[A\n",
      " 92%|█████████▏| 227/248 [35:30<02:21,  6.74s/it]\u001B[A\n",
      " 92%|█████████▏| 228/248 [35:35<02:08,  6.41s/it]\u001B[A\n",
      " 92%|█████████▏| 229/248 [35:40<01:49,  5.75s/it]\u001B[A\n",
      " 93%|█████████▎| 230/248 [35:46<01:46,  5.93s/it]\u001B[A\n",
      " 93%|█████████▎| 231/248 [35:51<01:37,  5.74s/it]\u001B[A\n",
      " 94%|█████████▎| 232/248 [35:56<01:26,  5.40s/it]\u001B[A\n",
      " 94%|█████████▍| 233/248 [36:01<01:17,  5.16s/it]\u001B[A\n",
      " 94%|█████████▍| 234/248 [36:06<01:15,  5.38s/it]\u001B[A\n",
      " 95%|█████████▍| 235/248 [36:12<01:08,  5.29s/it]\u001B[A\n",
      " 95%|█████████▌| 236/248 [36:18<01:06,  5.51s/it]\u001B[A\n",
      " 96%|█████████▌| 237/248 [36:22<00:58,  5.32s/it]\u001B[A\n",
      " 96%|█████████▌| 238/248 [36:28<00:54,  5.42s/it]\u001B[A\n",
      " 96%|█████████▋| 239/248 [36:35<00:51,  5.78s/it]\u001B[A\n",
      " 97%|█████████▋| 240/248 [36:40<00:45,  5.69s/it]\u001B[A\n",
      " 97%|█████████▋| 241/248 [36:46<00:39,  5.67s/it]\u001B[A\n",
      " 98%|█████████▊| 242/248 [36:51<00:33,  5.52s/it]\u001B[A\n",
      " 98%|█████████▊| 243/248 [36:56<00:26,  5.25s/it]\u001B[A\n",
      " 98%|█████████▊| 244/248 [37:01<00:20,  5.24s/it]\u001B[A\n",
      " 99%|█████████▉| 245/248 [37:06<00:15,  5.22s/it]\u001B[A\n",
      " 99%|█████████▉| 246/248 [37:11<00:10,  5.29s/it]\u001B[A\n",
      "100%|█████████▉| 247/248 [37:20<00:06,  6.26s/it]\u001B[A\n",
      "100%|██████████| 248/248 [37:26<00:00,  9.06s/it]\u001B[A\n",
      "02/09/2023 01:03:55 - INFO - __main__ -   ***** Eval results  *****\n",
      "02/09/2023 01:03:55 - INFO - __main__ -     Loss = 279050955.02576244\n",
      "02/09/2023 01:03:55 - INFO - __main__ -    global_step = 500, evaluate loss = 279050955.02576244\n",
      "02/09/2023 01:03:56 - INFO - __main__ -   Saving model checkpoint to ./debiased_models/gpt2\\debiased-checkpoint-best\n",
      "02/09/2023 01:03:58 - INFO - __main__ -   Saving optimizer and scheduler states to ./debiased_models/gpt2\\debiased-checkpoint-best\n",
      "02/09/2023 01:03:58 - INFO - __main__ -    best_step = 500, best loss = 279050955.02576244\n",
      "Iteration: 100%|██████████| 632/632 [3:38:24<00:00, 20.73s/it]    \n",
      "Iteration:  58%|█████▊    | 367/632 [1:52:40<1:04:10, 14.53s/it]02/09/2023 03:33:43 - INFO - __main__ -    global_step = 1000, train loss = 527091042.10239327\n",
      "02/09/2023 03:33:43 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "02/09/2023 03:33:43 - INFO - __main__ -     Num examples = 8000\n",
      "02/09/2023 03:33:43 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "  0%|          | 0/248 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/248 [00:09<37:34,  9.13s/it]\u001B[A\n",
      "  1%|          | 2/248 [00:19<39:44,  9.69s/it]\u001B[A\n",
      "  1%|          | 3/248 [00:29<40:04,  9.81s/it]\u001B[A\n",
      "  2%|▏         | 4/248 [00:49<56:10, 13.81s/it]\u001B[A\n",
      "  2%|▏         | 5/248 [00:59<51:04, 12.61s/it]\u001B[A\n",
      "  2%|▏         | 6/248 [01:09<47:23, 11.75s/it]\u001B[A\n",
      "  3%|▎         | 7/248 [01:24<51:28, 12.82s/it]\u001B[A\n",
      "  3%|▎         | 8/248 [01:34<46:56, 11.74s/it]\u001B[A\n",
      "  4%|▎         | 9/248 [01:43<43:50, 11.01s/it]\u001B[A\n",
      "  4%|▍         | 10/248 [01:55<44:25, 11.20s/it]\u001B[A\n",
      "  4%|▍         | 11/248 [02:05<43:19, 10.97s/it]\u001B[A\n",
      "  5%|▍         | 12/248 [02:19<46:52, 11.92s/it]\u001B[A\n",
      "  5%|▌         | 13/248 [02:33<48:43, 12.44s/it]\u001B[A\n",
      "  6%|▌         | 14/248 [02:41<44:01, 11.29s/it]\u001B[A\n",
      "  6%|▌         | 15/248 [02:53<44:15, 11.39s/it]\u001B[A\n",
      "  6%|▋         | 16/248 [03:05<44:23, 11.48s/it]\u001B[A\n",
      "  7%|▋         | 17/248 [03:16<43:41, 11.35s/it]\u001B[A\n",
      "  7%|▋         | 18/248 [03:31<47:53, 12.49s/it]\u001B[A\n",
      "  8%|▊         | 19/248 [03:41<45:21, 11.88s/it]\u001B[A\n",
      "  8%|▊         | 20/248 [03:52<43:51, 11.54s/it]\u001B[A\n",
      "  8%|▊         | 21/248 [04:15<56:10, 14.85s/it]\u001B[A\n",
      "  9%|▉         | 22/248 [04:25<50:30, 13.41s/it]\u001B[A\n",
      "  9%|▉         | 23/248 [04:35<46:08, 12.30s/it]\u001B[A\n",
      " 10%|▉         | 24/248 [04:46<45:31, 12.20s/it]\u001B[A\n",
      " 10%|█         | 25/248 [04:58<44:51, 12.07s/it]\u001B[A\n",
      " 10%|█         | 26/248 [05:13<47:40, 12.88s/it]\u001B[A\n",
      " 11%|█         | 27/248 [05:23<44:38, 12.12s/it]\u001B[A\n",
      " 11%|█▏        | 28/248 [05:35<43:47, 11.94s/it]\u001B[A\n",
      " 12%|█▏        | 29/248 [05:49<45:34, 12.49s/it]\u001B[A\n",
      " 12%|█▏        | 30/248 [05:58<41:35, 11.45s/it]\u001B[A\n",
      " 12%|█▎        | 31/248 [06:16<48:37, 13.45s/it]\u001B[A\n",
      " 13%|█▎        | 32/248 [06:26<44:30, 12.36s/it]\u001B[A\n",
      " 13%|█▎        | 33/248 [06:37<43:28, 12.13s/it]\u001B[A\n",
      " 14%|█▎        | 34/248 [06:48<41:19, 11.58s/it]\u001B[A\n",
      " 14%|█▍        | 35/248 [07:10<52:55, 14.91s/it]\u001B[A\n",
      " 15%|█▍        | 36/248 [07:21<48:49, 13.82s/it]\u001B[A\n",
      " 15%|█▍        | 37/248 [07:34<46:58, 13.36s/it]\u001B[A\n",
      " 15%|█▌        | 38/248 [07:48<47:37, 13.61s/it]\u001B[A\n",
      " 16%|█▌        | 39/248 [08:01<46:40, 13.40s/it]\u001B[A\n",
      " 16%|█▌        | 40/248 [08:12<43:44, 12.62s/it]\u001B[A\n",
      " 17%|█▋        | 41/248 [08:23<42:38, 12.36s/it]\u001B[A\n",
      " 17%|█▋        | 42/248 [08:31<37:56, 11.05s/it]\u001B[A\n",
      " 17%|█▋        | 43/248 [08:42<37:12, 10.89s/it]\u001B[A\n",
      " 18%|█▊        | 44/248 [08:52<36:06, 10.62s/it]\u001B[A\n",
      " 18%|█▊        | 45/248 [09:05<38:51, 11.48s/it]\u001B[A\n",
      " 19%|█▊        | 46/248 [09:20<41:37, 12.36s/it]\u001B[A\n",
      " 19%|█▉        | 47/248 [09:38<46:56, 14.01s/it]\u001B[A\n",
      " 19%|█▉        | 48/248 [09:51<45:37, 13.69s/it]\u001B[A\n",
      " 20%|█▉        | 49/248 [10:02<43:13, 13.03s/it]\u001B[A\n",
      " 20%|██        | 50/248 [10:14<41:44, 12.65s/it]\u001B[A\n",
      " 21%|██        | 51/248 [10:25<40:32, 12.35s/it]\u001B[A\n",
      " 21%|██        | 52/248 [10:37<39:57, 12.23s/it]\u001B[A\n",
      " 21%|██▏       | 53/248 [10:48<37:59, 11.69s/it]\u001B[A\n",
      " 22%|██▏       | 54/248 [11:00<38:22, 11.87s/it]\u001B[A\n",
      " 22%|██▏       | 55/248 [11:15<40:43, 12.66s/it]\u001B[A\n",
      " 23%|██▎       | 56/248 [11:27<39:51, 12.46s/it]\u001B[A\n",
      " 23%|██▎       | 57/248 [11:39<39:13, 12.32s/it]\u001B[A\n",
      " 23%|██▎       | 58/248 [11:51<38:49, 12.26s/it]\u001B[A\n",
      " 24%|██▍       | 59/248 [12:00<35:44, 11.35s/it]\u001B[A\n",
      " 24%|██▍       | 60/248 [12:12<35:52, 11.45s/it]\u001B[A\n",
      " 25%|██▍       | 61/248 [12:29<41:04, 13.18s/it]\u001B[A\n",
      " 25%|██▌       | 62/248 [12:39<38:17, 12.35s/it]\u001B[A\n",
      " 25%|██▌       | 63/248 [12:56<42:25, 13.76s/it]\u001B[A\n",
      " 26%|██▌       | 64/248 [13:08<40:38, 13.25s/it]\u001B[A\n",
      " 26%|██▌       | 65/248 [13:18<36:42, 12.03s/it]\u001B[A\n",
      " 27%|██▋       | 66/248 [13:28<34:55, 11.51s/it]\u001B[A\n",
      " 27%|██▋       | 67/248 [13:39<34:19, 11.38s/it]\u001B[A\n",
      " 27%|██▋       | 68/248 [13:50<33:50, 11.28s/it]\u001B[A\n",
      " 28%|██▊       | 69/248 [14:02<34:21, 11.52s/it]\u001B[A\n",
      " 28%|██▊       | 70/248 [14:12<32:20, 10.90s/it]\u001B[A\n",
      " 29%|██▊       | 71/248 [14:34<42:17, 14.34s/it]\u001B[A\n",
      " 29%|██▉       | 72/248 [14:45<39:30, 13.47s/it]\u001B[A\n",
      " 29%|██▉       | 73/248 [14:56<36:39, 12.57s/it]\u001B[A\n",
      " 30%|██▉       | 74/248 [15:07<35:32, 12.26s/it]\u001B[A\n",
      " 30%|███       | 75/248 [15:19<35:01, 12.14s/it]\u001B[A\n",
      " 31%|███       | 76/248 [15:37<39:47, 13.88s/it]\u001B[A\n",
      " 31%|███       | 77/248 [15:47<35:54, 12.60s/it]\u001B[A\n",
      " 31%|███▏      | 78/248 [15:59<35:32, 12.55s/it]\u001B[A\n",
      " 32%|███▏      | 79/248 [16:14<36:53, 13.10s/it]\u001B[A\n",
      " 32%|███▏      | 80/248 [16:28<37:35, 13.43s/it]\u001B[A\n",
      " 33%|███▎      | 81/248 [16:40<36:17, 13.04s/it]\u001B[A\n",
      " 33%|███▎      | 82/248 [16:53<36:22, 13.15s/it]\u001B[A\n",
      " 33%|███▎      | 83/248 [17:04<33:50, 12.31s/it]\u001B[A\n",
      " 34%|███▍      | 84/248 [17:16<33:36, 12.29s/it]\u001B[A\n",
      " 34%|███▍      | 85/248 [17:29<34:24, 12.66s/it]\u001B[A\n",
      " 35%|███▍      | 86/248 [17:39<31:18, 11.60s/it]\u001B[A\n",
      " 35%|███▌      | 87/248 [17:49<29:53, 11.14s/it]\u001B[A\n",
      " 35%|███▌      | 88/248 [18:01<30:28, 11.43s/it]\u001B[A\n",
      " 36%|███▌      | 89/248 [18:12<30:18, 11.44s/it]\u001B[A\n",
      " 36%|███▋      | 90/248 [18:22<29:01, 11.02s/it]\u001B[A\n",
      " 37%|███▋      | 91/248 [18:36<30:36, 11.70s/it]\u001B[A\n",
      " 37%|███▋      | 92/248 [18:48<30:38, 11.78s/it]\u001B[A\n",
      " 38%|███▊      | 93/248 [18:59<30:31, 11.82s/it]\u001B[A\n",
      " 38%|███▊      | 94/248 [19:11<30:26, 11.86s/it]\u001B[A\n",
      " 38%|███▊      | 95/248 [19:23<29:44, 11.66s/it]\u001B[A\n",
      " 39%|███▊      | 96/248 [19:33<28:43, 11.34s/it]\u001B[A\n",
      " 39%|███▉      | 97/248 [19:44<27:54, 11.09s/it]\u001B[A\n",
      " 40%|███▉      | 98/248 [19:54<27:14, 10.90s/it]\u001B[A\n",
      " 40%|███▉      | 99/248 [20:06<27:51, 11.22s/it]\u001B[A\n",
      " 40%|████      | 100/248 [20:17<27:42, 11.23s/it]\u001B[A\n",
      " 41%|████      | 101/248 [20:28<27:04, 11.05s/it]\u001B[A\n",
      " 41%|████      | 102/248 [20:45<30:57, 12.72s/it]\u001B[A\n",
      " 42%|████▏     | 103/248 [20:56<29:37, 12.26s/it]\u001B[A\n",
      " 42%|████▏     | 104/248 [21:07<28:58, 12.07s/it]\u001B[A\n",
      " 42%|████▏     | 105/248 [21:19<28:19, 11.89s/it]\u001B[A\n",
      " 43%|████▎     | 106/248 [21:34<30:37, 12.94s/it]\u001B[A\n",
      " 43%|████▎     | 107/248 [21:49<31:43, 13.50s/it]\u001B[A\n",
      " 44%|████▎     | 108/248 [21:59<29:19, 12.57s/it]\u001B[A\n",
      " 44%|████▍     | 109/248 [22:10<27:55, 12.05s/it]\u001B[A\n",
      " 44%|████▍     | 110/248 [22:21<26:44, 11.62s/it]\u001B[A\n",
      " 45%|████▍     | 111/248 [22:32<26:01, 11.40s/it]\u001B[A\n",
      " 45%|████▌     | 112/248 [22:54<33:09, 14.63s/it]\u001B[A\n",
      " 46%|████▌     | 113/248 [23:06<31:22, 13.94s/it]\u001B[A\n",
      " 46%|████▌     | 114/248 [23:23<32:41, 14.64s/it]\u001B[A\n",
      " 46%|████▋     | 115/248 [23:39<33:54, 15.30s/it]\u001B[A\n",
      " 47%|████▋     | 116/248 [23:51<30:59, 14.09s/it]\u001B[A\n",
      " 47%|████▋     | 117/248 [24:05<31:11, 14.29s/it]\u001B[A\n",
      " 48%|████▊     | 118/248 [24:17<29:00, 13.39s/it]\u001B[A\n",
      " 48%|████▊     | 119/248 [24:28<27:37, 12.85s/it]\u001B[A\n",
      " 48%|████▊     | 120/248 [24:43<28:17, 13.26s/it]\u001B[A\n",
      " 49%|████▉     | 121/248 [24:53<26:31, 12.53s/it]\u001B[A\n",
      " 49%|████▉     | 122/248 [25:05<25:27, 12.12s/it]\u001B[A\n",
      " 50%|████▉     | 123/248 [25:14<23:20, 11.20s/it]\u001B[A\n",
      " 50%|█████     | 124/248 [25:26<23:40, 11.46s/it]\u001B[A\n",
      " 50%|█████     | 125/248 [25:31<19:51,  9.69s/it]\u001B[A\n",
      " 51%|█████     | 126/248 [25:37<17:12,  8.46s/it]\u001B[A\n",
      " 51%|█████     | 127/248 [25:42<14:47,  7.34s/it]\u001B[A\n",
      " 52%|█████▏    | 128/248 [25:46<13:03,  6.53s/it]\u001B[A\n",
      " 52%|█████▏    | 129/248 [25:53<12:56,  6.53s/it]\u001B[A\n",
      " 52%|█████▏    | 130/248 [25:57<11:45,  5.98s/it]\u001B[A\n",
      " 53%|█████▎    | 131/248 [26:06<13:01,  6.68s/it]\u001B[A\n",
      " 53%|█████▎    | 132/248 [26:12<12:46,  6.61s/it]\u001B[A\n",
      " 54%|█████▎    | 133/248 [26:17<11:40,  6.09s/it]\u001B[A\n",
      " 54%|█████▍    | 134/248 [26:23<11:45,  6.19s/it]\u001B[A\n",
      " 54%|█████▍    | 135/248 [26:29<11:22,  6.04s/it]\u001B[A\n",
      " 55%|█████▍    | 136/248 [26:34<10:24,  5.58s/it]\u001B[A\n",
      " 55%|█████▌    | 137/248 [26:38<09:36,  5.19s/it]\u001B[A\n",
      " 56%|█████▌    | 138/248 [26:48<12:10,  6.64s/it]\u001B[A\n",
      " 56%|█████▌    | 139/248 [26:54<11:38,  6.41s/it]\u001B[A\n",
      " 56%|█████▋    | 140/248 [27:00<11:28,  6.38s/it]\u001B[A\n",
      " 57%|█████▋    | 141/248 [27:07<11:37,  6.52s/it]\u001B[A\n",
      " 57%|█████▋    | 142/248 [27:15<12:08,  6.87s/it]\u001B[A\n",
      " 58%|█████▊    | 143/248 [27:21<11:52,  6.78s/it]\u001B[A\n",
      " 58%|█████▊    | 144/248 [27:27<11:12,  6.47s/it]\u001B[A\n",
      " 58%|█████▊    | 145/248 [27:33<10:42,  6.24s/it]\u001B[A\n",
      " 59%|█████▉    | 146/248 [27:37<09:39,  5.68s/it]\u001B[A\n",
      " 59%|█████▉    | 147/248 [27:42<09:16,  5.51s/it]\u001B[A\n",
      " 60%|█████▉    | 148/248 [27:49<09:37,  5.78s/it]\u001B[A\n",
      " 60%|██████    | 149/248 [27:53<08:55,  5.41s/it]\u001B[A\n",
      " 60%|██████    | 150/248 [27:59<08:57,  5.48s/it]\u001B[A\n",
      " 61%|██████    | 151/248 [28:03<08:26,  5.23s/it]\u001B[A\n",
      " 61%|██████▏   | 152/248 [28:11<09:38,  6.02s/it]\u001B[A\n",
      " 62%|██████▏   | 153/248 [28:19<10:18,  6.51s/it]\u001B[A\n",
      " 62%|██████▏   | 154/248 [28:24<09:41,  6.18s/it]\u001B[A\n",
      " 62%|██████▎   | 155/248 [28:29<09:04,  5.86s/it]\u001B[A\n",
      " 63%|██████▎   | 156/248 [28:35<08:43,  5.69s/it]\u001B[A\n",
      " 63%|██████▎   | 157/248 [28:38<07:40,  5.06s/it]\u001B[A\n",
      " 64%|██████▎   | 158/248 [28:44<07:49,  5.22s/it]\u001B[A\n",
      " 64%|██████▍   | 159/248 [28:49<07:45,  5.23s/it]\u001B[A\n",
      " 65%|██████▍   | 160/248 [28:59<09:45,  6.65s/it]\u001B[A\n",
      " 65%|██████▍   | 161/248 [29:06<09:44,  6.72s/it]\u001B[A\n",
      " 65%|██████▌   | 162/248 [29:12<09:24,  6.56s/it]\u001B[A\n",
      " 66%|██████▌   | 163/248 [29:16<08:17,  5.86s/it]\u001B[A\n",
      " 66%|██████▌   | 164/248 [29:22<08:14,  5.88s/it]\u001B[A\n",
      " 67%|██████▋   | 165/248 [29:27<07:31,  5.44s/it]\u001B[A\n",
      " 67%|██████▋   | 166/248 [29:31<06:51,  5.02s/it]\u001B[A\n",
      " 67%|██████▋   | 167/248 [29:35<06:33,  4.86s/it]\u001B[A\n",
      " 68%|██████▊   | 168/248 [29:40<06:23,  4.79s/it]\u001B[A\n",
      " 68%|██████▊   | 169/248 [29:45<06:16,  4.76s/it]\u001B[A\n",
      " 69%|██████▊   | 170/248 [29:50<06:34,  5.06s/it]\u001B[A\n",
      " 69%|██████▉   | 171/248 [29:56<06:48,  5.30s/it]\u001B[A\n",
      " 69%|██████▉   | 172/248 [30:02<06:42,  5.30s/it]\u001B[A\n",
      " 70%|██████▉   | 173/248 [30:07<06:29,  5.20s/it]\u001B[A\n",
      " 70%|███████   | 174/248 [30:12<06:26,  5.23s/it]\u001B[A\n",
      " 71%|███████   | 175/248 [30:17<06:16,  5.16s/it]\u001B[A\n",
      " 71%|███████   | 176/248 [30:22<06:15,  5.22s/it]\u001B[A\n",
      " 71%|███████▏  | 177/248 [30:28<06:20,  5.36s/it]\u001B[A\n",
      " 72%|███████▏  | 178/248 [30:36<07:08,  6.12s/it]\u001B[A\n",
      " 72%|███████▏  | 179/248 [30:42<07:12,  6.27s/it]\u001B[A\n",
      " 73%|███████▎  | 180/248 [30:48<06:44,  5.95s/it]\u001B[A\n",
      " 73%|███████▎  | 181/248 [30:52<06:16,  5.62s/it]\u001B[A\n",
      " 73%|███████▎  | 182/248 [30:58<06:00,  5.45s/it]\u001B[A\n",
      " 74%|███████▍  | 183/248 [31:03<05:50,  5.39s/it]\u001B[A\n",
      " 74%|███████▍  | 184/248 [31:07<05:20,  5.01s/it]\u001B[A\n",
      " 75%|███████▍  | 185/248 [31:14<05:47,  5.51s/it]\u001B[A\n",
      " 75%|███████▌  | 186/248 [31:20<05:51,  5.67s/it]\u001B[A\n",
      " 75%|███████▌  | 187/248 [31:25<05:42,  5.62s/it]\u001B[A\n",
      " 76%|███████▌  | 188/248 [31:30<05:26,  5.45s/it]\u001B[A\n",
      " 76%|███████▌  | 189/248 [31:36<05:22,  5.47s/it]\u001B[A\n",
      " 77%|███████▋  | 190/248 [31:42<05:34,  5.77s/it]\u001B[A\n",
      " 77%|███████▋  | 191/248 [31:50<06:01,  6.34s/it]\u001B[A\n",
      " 77%|███████▋  | 192/248 [32:01<07:08,  7.65s/it]\u001B[A\n",
      " 78%|███████▊  | 193/248 [32:05<06:16,  6.84s/it]\u001B[A\n",
      " 78%|███████▊  | 194/248 [32:10<05:37,  6.24s/it]\u001B[A\n",
      " 79%|███████▊  | 195/248 [32:16<05:21,  6.07s/it]\u001B[A\n",
      " 79%|███████▉  | 196/248 [32:22<05:20,  6.16s/it]\u001B[A\n",
      " 79%|███████▉  | 197/248 [32:27<04:53,  5.75s/it]\u001B[A\n",
      " 80%|███████▉  | 198/248 [32:32<04:27,  5.35s/it]\u001B[A\n",
      " 80%|████████  | 199/248 [32:36<04:03,  4.97s/it]\u001B[A\n",
      " 81%|████████  | 200/248 [32:40<03:47,  4.74s/it]\u001B[A\n",
      " 81%|████████  | 201/248 [32:45<03:44,  4.77s/it]\u001B[A\n",
      " 81%|████████▏ | 202/248 [32:51<04:04,  5.31s/it]\u001B[A\n",
      " 82%|████████▏ | 203/248 [32:55<03:43,  4.97s/it]\u001B[A\n",
      " 82%|████████▏ | 204/248 [33:01<03:40,  5.01s/it]\u001B[A\n",
      " 83%|████████▎ | 205/248 [33:06<03:47,  5.28s/it]\u001B[A\n",
      " 83%|████████▎ | 206/248 [33:13<03:53,  5.56s/it]\u001B[A\n",
      " 83%|████████▎ | 207/248 [33:18<03:45,  5.51s/it]\u001B[A\n",
      " 84%|████████▍ | 208/248 [33:24<03:44,  5.61s/it]\u001B[A\n",
      " 84%|████████▍ | 209/248 [33:30<03:40,  5.64s/it]\u001B[A\n",
      " 85%|████████▍ | 210/248 [33:35<03:26,  5.45s/it]\u001B[A\n",
      " 85%|████████▌ | 211/248 [33:42<03:37,  5.88s/it]\u001B[A\n",
      " 85%|████████▌ | 212/248 [33:47<03:30,  5.85s/it]\u001B[A\n",
      " 86%|████████▌ | 213/248 [33:54<03:31,  6.04s/it]\u001B[A\n",
      " 86%|████████▋ | 214/248 [34:01<03:39,  6.45s/it]\u001B[A\n",
      " 87%|████████▋ | 215/248 [34:07<03:23,  6.15s/it]\u001B[A\n",
      " 87%|████████▋ | 216/248 [34:13<03:16,  6.13s/it]\u001B[A\n",
      " 88%|████████▊ | 217/248 [34:18<03:01,  5.85s/it]\u001B[A\n",
      " 88%|████████▊ | 218/248 [34:24<02:57,  5.91s/it]\u001B[A\n",
      " 88%|████████▊ | 219/248 [34:29<02:43,  5.63s/it]\u001B[A\n",
      " 89%|████████▊ | 220/248 [34:34<02:35,  5.55s/it]\u001B[A\n",
      " 89%|████████▉ | 221/248 [34:41<02:36,  5.81s/it]\u001B[A\n",
      " 90%|████████▉ | 222/248 [34:46<02:25,  5.61s/it]\u001B[A\n",
      " 90%|████████▉ | 223/248 [34:51<02:15,  5.44s/it]\u001B[A\n",
      " 90%|█████████ | 224/248 [34:59<02:32,  6.37s/it]\u001B[A\n",
      " 91%|█████████ | 225/248 [35:05<02:18,  6.01s/it]\u001B[A\n",
      " 91%|█████████ | 226/248 [35:10<02:07,  5.81s/it]\u001B[A\n",
      " 92%|█████████▏| 227/248 [35:18<02:18,  6.59s/it]\u001B[A\n",
      " 92%|█████████▏| 228/248 [35:23<02:01,  6.09s/it]\u001B[A\n",
      " 92%|█████████▏| 229/248 [35:28<01:47,  5.68s/it]\u001B[A\n",
      " 93%|█████████▎| 230/248 [35:32<01:35,  5.31s/it]\u001B[A\n",
      " 93%|█████████▎| 231/248 [35:37<01:28,  5.23s/it]\u001B[A\n",
      " 94%|█████████▎| 232/248 [35:43<01:24,  5.27s/it]\u001B[A\n",
      " 94%|█████████▍| 233/248 [35:52<01:35,  6.39s/it]\u001B[A\n",
      " 94%|█████████▍| 234/248 [35:58<01:27,  6.27s/it]\u001B[A\n",
      " 95%|█████████▍| 235/248 [36:04<01:20,  6.21s/it]\u001B[A\n",
      " 95%|█████████▌| 236/248 [36:10<01:12,  6.06s/it]\u001B[A\n",
      " 96%|█████████▌| 237/248 [36:15<01:03,  5.74s/it]\u001B[A\n",
      " 96%|█████████▌| 238/248 [36:19<00:53,  5.32s/it]\u001B[A\n",
      " 96%|█████████▋| 239/248 [36:25<00:49,  5.53s/it]\u001B[A\n",
      " 97%|█████████▋| 240/248 [36:32<00:46,  5.87s/it]\u001B[A\n",
      " 97%|█████████▋| 241/248 [36:36<00:38,  5.53s/it]\u001B[A\n",
      " 98%|█████████▊| 242/248 [36:42<00:33,  5.56s/it]\u001B[A\n",
      " 98%|█████████▊| 243/248 [36:47<00:27,  5.43s/it]\u001B[A\n",
      " 98%|█████████▊| 244/248 [36:52<00:20,  5.20s/it]\u001B[A\n",
      " 99%|█████████▉| 245/248 [36:57<00:15,  5.06s/it]\u001B[A\n",
      " 99%|█████████▉| 246/248 [37:01<00:10,  5.02s/it]\u001B[A\n",
      "100%|█████████▉| 247/248 [37:07<00:05,  5.30s/it]\u001B[A\n",
      "100%|██████████| 248/248 [37:12<00:00,  9.00s/it]\u001B[A\n",
      "02/09/2023 04:10:56 - INFO - __main__ -   ***** Eval results  *****\n",
      "02/09/2023 04:10:56 - INFO - __main__ -     Loss = 113421796.22157967\n",
      "02/09/2023 04:10:56 - INFO - __main__ -    global_step = 1000, evaluate loss = 113421796.22157967\n",
      "02/09/2023 04:10:57 - INFO - __main__ -   Saving model checkpoint to ./debiased_models/gpt2\\debiased-checkpoint-best\n",
      "02/09/2023 04:10:59 - INFO - __main__ -   Saving optimizer and scheduler states to ./debiased_models/gpt2\\debiased-checkpoint-best\n",
      "02/09/2023 04:10:59 - INFO - __main__ -    best_step = 1000, best loss = 113421796.22157967\n",
      "Iteration:  70%|███████   | 445/632 [2:48:57<38:42, 12.42s/it]    "
     ]
    }
   ],
   "source": [
    "if (\n",
    "    os.path.exists(args.output_dir)\n",
    "    and os.listdir(args.output_dir)\n",
    "    and args.do_train\n",
    "    and not args.overwrite_output_dir\n",
    "    and not args.should_continue\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "set_seed(args)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    " # Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "else:\n",
    "    # When we release a pip version exposing CONFIG_MAPPING,\n",
    "    # we can do `config = CONFIG_MAPPING[args.model_type]()`.\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --config_name\"\n",
    "    )\n",
    "\n",
    "config.output_hidden_states = 'true'\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --tokenizer_name\"\n",
    "    )\n",
    "\n",
    "if args.block_size <= 0:\n",
    "    args.block_size = tokenizer.model_max_length\n",
    "    # Our input block size will be the max possible for the model\n",
    "else:\n",
    "    args.block_size = min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    original_model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    original_model = AutoModelWithLMHead.from_config(config)\n",
    "\n",
    "# GPT-2 and GPT do not have pad.\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    original_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(args.device)\n",
    "original_model.to(args.device)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "attributes_examples=[]\n",
    "attributes_labels=[]\n",
    "neutral_examples=[]\n",
    "neutral_exist=0\n",
    "for data_file in args.data_files:\n",
    "    data=torch.load(data_file)\n",
    "    attributes_examples.append(data['attributes_examples'])\n",
    "    attributes_labels.append(data['attributes_labels'])\n",
    "    neutral_examples.append(data['neutral_examples'])\n",
    "\n",
    "    if 'neutral_labels' in data:\n",
    "        neutral_exist+=1\n",
    "\n",
    "if neutral_exist==len(args.data_files):\n",
    "    neutral_labels=[]\n",
    "    for data_file in args.data_files:\n",
    "        data=torch.load(data_file)\n",
    "        neutral_labels.append(data['neutral_labels'])\n",
    "    splited_data=split_data(attributes_examples, attributes_labels, neutral_examples, neutral_labels, args)\n",
    "else:\n",
    "    splited_data=split_data(attributes_examples, attributes_labels, neutral_examples, None, args)\n",
    "\n",
    "datasets = load_and_cache_examples(splited_data, args, tokenizer)\n",
    "\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "train(args, splited_data, datasets, model, original_model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}